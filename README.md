# DataLake Project

## Project Purpose:

The purpose im of this project is to provide an ETL pipeline for  the startup sparkify using a data lake to automate the data orchestration process combining multiple data sources from AWS S3 into structured data and loading processed data into respective Fact and dimentsion tables using spark and then upload the parquet files back to S3 for further data analysis. 


## Dataset (S3 buckets):
Song Data: s3://udacity-dend/song_data 
Log Data Path: s3://udacity-dend/log_data 

The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

The log dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. 

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

## Schema for Song Play Analysis
A Star Schema is recommended for optimized queries

## Fact Table
### songplays_table
records in log data associated with song plays i.e. records with page NextSong 
Fields: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

## Dimension Tables
### users_schema_table
users in the app 
Fields: user_id, first_name, last_name, gender, level

### songs_schema_table
songs in music database 
Fields: song_id, title, artist_id, year, duration

### artists_schema_table
artists in music database 
Fields: artist_id, name, location, lattitude, longitude

### time_schema_table
timestamps of records in songplays broken down into specific units 
Fields: start_time, hour, day, week, month, year, weekday

## Pipeline
1. Load the Data which are in JSON Files(Song Data and Log Data)
2. Use Spark to process these JSON files and then generate a set of Fact and Dimension Tables
3. Load back this data to S3


## Execution Steps:
1. Create an AWS IAM role with S3 read and write access.
2. Enter the IAM's credentials in the `dl.cfg` configuration file.
3. Create an S3 bucket and enter the URL to the bucket in `etl.py` as the value of output_data.
4. Run `python etl.py` to process the data and store it on your created S3 bucket.
